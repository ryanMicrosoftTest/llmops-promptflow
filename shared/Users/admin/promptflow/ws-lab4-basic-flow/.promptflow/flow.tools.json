{
  "package": {
    "promptflow.tools.azure_content_safety.analyze_text": {
      "name": "Content Safety (Text Analyze)",
      "type": "python",
      "inputs": {
        "connection": {
          "type": [
            "AzureContentSafetyConnection"
          ],
          "allow_manual_entry": false,
          "is_multi_select": false,
          "input_type": "default"
        },
        "hate_category": {
          "type": [
            "string"
          ],
          "default": "medium_sensitivity",
          "enum": [
            "disable",
            "low_sensitivity",
            "medium_sensitivity",
            "high_sensitivity"
          ],
          "allow_manual_entry": false,
          "is_multi_select": false,
          "input_type": "default"
        },
        "self_harm_category": {
          "type": [
            "string"
          ],
          "default": "medium_sensitivity",
          "enum": [
            "disable",
            "low_sensitivity",
            "medium_sensitivity",
            "high_sensitivity"
          ],
          "allow_manual_entry": false,
          "is_multi_select": false,
          "input_type": "default"
        },
        "sexual_category": {
          "type": [
            "string"
          ],
          "default": "medium_sensitivity",
          "enum": [
            "disable",
            "low_sensitivity",
            "medium_sensitivity",
            "high_sensitivity"
          ],
          "allow_manual_entry": false,
          "is_multi_select": false,
          "input_type": "default"
        },
        "text": {
          "type": [
            "string"
          ],
          "allow_manual_entry": false,
          "is_multi_select": false,
          "input_type": "default"
        },
        "violence_category": {
          "type": [
            "string"
          ],
          "default": "medium_sensitivity",
          "enum": [
            "disable",
            "low_sensitivity",
            "medium_sensitivity",
            "high_sensitivity"
          ],
          "allow_manual_entry": false,
          "is_multi_select": false,
          "input_type": "default"
        }
      },
      "description": "Use Azure Content Safety to detect harmful content.",
      "module": "promptflow.tools.azure_content_safety",
      "function": "analyze_text",
      "is_builtin": true,
      "package": "promptflow-tools",
      "package_version": "1.5.0rc11",
      "enable_kwargs": false,
      "deprecated_tools": [
        "content_safety_text.tools.content_safety_text_tool.analyze_text"
      ],
      "tool_state": "stable",
      "toolId": "promptflow.tools.azure_content_safety.analyze_text"
    }
  },
  "code": {
    "echo.py": {
      "type": "python",
      "inputs": {
        "input": {
          "type": [
            "string"
          ]
        }
      },
      "source": "echo.py",
      "function": "echo"
    },
    "joke.jinja2": {
      "type": "llm",
      "inputs": {
        "topic": {
          "type": [
            "string"
          ]
        }
      },
      "description": "Prompt is a jinja2 template that generates prompt for LLM",
      "source": "joke.jinja2"
    },
    "contentOutput.py": {
      "type": "python",
      "inputs": {
        "input1": {
          "type": [
            "object"
          ]
        }
      },
      "source": "contentOutput.py",
      "function": "proceed_with_joke"
    }
  }
}